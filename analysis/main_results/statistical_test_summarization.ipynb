{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pingouin import wilcoxon\n",
    "from tqdm import tqdm\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"summarization\"\n",
    "NAME_MAPPING = {\n",
    "    \"codet5p-220m\": \"CodeT5+ 220M\",\n",
    "    \"codet5p-770m\": \"CodeT5+ 770M\"\n",
    "}\n",
    "RENAME_TUNING_METHOD_DICT = {\n",
    "    \"full-finetuning\": \"Full Fine-Tuning\",\n",
    "    \"no-gnn\": \"Linear Adapter\",\n",
    "    \"concatpervector\": \"Transducer Tuning\",\n",
    "    \"lora\": \"LoRA\",\n",
    "    \"prompt-tuning\": \"Prompt-Tuning\",\n",
    "    \"prefix-tuning\": \"Prefix-Tuning\",\n",
    "    \"no-finetuning\": \"No Fine-Tuning\"\n",
    "}\n",
    "\n",
    "SEEDS = (\"seed_18_1\", \"seed_99_1\")\n",
    "DATASET_BASEPATH = \"/data/datasets/fix/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summarization/codet5p-220m_prompt-tuning.csv',\n",
       " 'summarization/codet5p-220m_concatpervector.csv',\n",
       " 'summarization/codet5p-770m_full-finetuning.csv',\n",
       " 'summarization/codet5p-770m_lora.csv',\n",
       " 'summarization/codet5p-220m_prefix-tuning.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(root_path: str)->list:\n",
    "    output_paths = []\n",
    "    for filename in os.listdir(root_path):\n",
    "        filepath = os.path.join(root_path, filename)\n",
    "        output_paths.append(filepath)\n",
    "    return output_paths\n",
    "\n",
    "output_paths = read_csv(TASK)\n",
    "output_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(paths: list)->pd.DataFrame:\n",
    "    temp_list = []\n",
    "    for path in paths:\n",
    "        if \"ipynb_checkpoints\" not in path:\n",
    "            filename = os.path.basename(path)\n",
    "            filename = filename.split(\"_\")\n",
    "            model = filename[0]\n",
    "            if model in NAME_MAPPING.keys():\n",
    "                \n",
    "                df = pd.read_csv(path)\n",
    "\n",
    "                task = path.split(\"/\")[0]\n",
    "                df[\"task\"] = task\n",
    "\n",
    "                # filter seed and similar ids\n",
    "                temp_task = task if task != \"code_repair\" else \"code_repair_long\"\n",
    "                ids_path = os.path.join(DATASET_BASEPATH, f\"{temp_task}/included_ids.csv\")\n",
    "                included_ids = pd.read_csv(ids_path)\n",
    "                mask_ids = df[\"idx.1\"].isin(included_ids[\"idx\"])\n",
    "                mask_seed = df[\"seed\"].isin(SEEDS)\n",
    "                df = df[(mask_seed) & (mask_ids)].copy()\n",
    "                df[\"model\"] = model\n",
    "\n",
    "                tuning_method = os.path.splitext(filename[1])[0]\n",
    "                df[\"tuning_method\"] = tuning_method\n",
    "\n",
    "                if task != \"summarization\":\n",
    "                    # df.drop(columns=[\"codebleu_stat\"], inplace=True)\n",
    "                    df = df[[\"model\", \"tuning_method\", \"task\", \"seed\", \"codebleu-cn\"]].copy()\n",
    "                    df.rename(columns={\"codebleu-cn\":\"codebleu\"}, inplace=True)\n",
    "                else:\n",
    "                    df = df[[\"model\", \"tuning_method\", \"task\",  \"seed\", \"bleu-cn\"]].copy().round(2)\n",
    "\n",
    "                temp_list.append(df)\n",
    "\n",
    "    df = pd.concat(temp_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>bleu-cn_mean</th>\n",
       "      <th>bleu-cn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>99.84</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>95.49</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>98.05</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>99.93</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>98.11</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>99.81</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>99.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>87.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>98.24</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>99.85</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>99.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  bleu-cn_mean  bleu-cn_std\n",
       "0    codet5p-220m  Transducer Tuning         99.84         0.21\n",
       "1    codet5p-220m   Full Fine-Tuning         99.91         0.01\n",
       "2    codet5p-220m               LoRA         99.91         0.00\n",
       "3    codet5p-220m     No Fine-Tuning         95.49         0.00\n",
       "4    codet5p-220m     Linear Adapter         98.05         0.88\n",
       "5    codet5p-220m      Prefix-Tuning         99.93         0.01\n",
       "6    codet5p-220m      Prompt-Tuning         99.91         0.01\n",
       "7    codet5p-770m  Transducer Tuning         98.11         1.61\n",
       "8    codet5p-770m   Full Fine-Tuning         99.81         0.01\n",
       "9    codet5p-770m               LoRA         99.79         0.02\n",
       "10   codet5p-770m     No Fine-Tuning         87.90         0.00\n",
       "11   codet5p-770m     Linear Adapter         98.24         1.39\n",
       "12   codet5p-770m      Prefix-Tuning         99.85         0.00\n",
       "13   codet5p-770m      Prompt-Tuning         99.82         0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_df(output_paths)\n",
    "\n",
    "temp_df = df.groupby([\"model\", \"tuning_method\", \"task\", \"seed\"], as_index=False).mean()\n",
    "temp_std = temp_df.groupby([\"model\", \"tuning_method\", \"task\"], as_index=False)[\"bleu-cn\"].std().round(2)\n",
    "\n",
    "df.drop(columns=[\"seed\"], inplace=True)\n",
    "\n",
    "# Calculate the mean and standard deviation for each group\n",
    "temp_mean = df.groupby([\"model\", \"tuning_method\", \"task\"], as_index=False).mean().round(2)\n",
    "\n",
    "# Add a suffix to the columns to distinguish between mean and std\n",
    "temp_mean = temp_mean.add_suffix('_mean')\n",
    "temp_std = temp_std.add_suffix('_std')\n",
    "\n",
    "# Merge mean and std DataFrames\n",
    "df_metric = pd.merge(temp_mean, temp_std, left_on=[\"model_mean\", \"tuning_method_mean\", \"task_mean\"], \n",
    "                         right_on=[\"model_std\", \"tuning_method_std\", \"task_std\"])\n",
    "\n",
    "# Drop redundant columns after merge\n",
    "df_metric.drop(columns=[\"model_std\", \"tuning_method_std\", \"task_std\", \"task_mean\"], inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_metric.rename(columns={\"model_mean\": \"backbone_model\", \"tuning_method_mean\": \"tuning_method\"} , inplace=True)\n",
    "df_metric[\"tuning_method\"] = df_metric[\"tuning_method\"].apply(lambda x: RENAME_TUNING_METHOD_DICT[x])\n",
    "\n",
    "\n",
    "df_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Trainable Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"codet5p-220m\": {\n",
    "        \"Transducer Tuning\": 30728,\n",
    "        \"Prefix-Tuning\": 184320,\n",
    "        \"Prompt-Tuning\": 38400,\n",
    "        \"LoRA\": 884736,\n",
    "        \"Full Fine-Tuning\": 222882048,\n",
    "        \"Linear Adapter\": 589824\n",
    "    },\n",
    "    \"codet5p-770m\": {\n",
    "        \"Transducer Tuning\": 37128,\n",
    "        \"Prefix-Tuning\": 491520,\n",
    "        \"Prompt-Tuning\": 102400,\n",
    "        \"LoRA\": 2359296,\n",
    "        \"Full Fine-Tuning\": 737639424,\n",
    "        \"Linear Adapter\": 1048576\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new dictionary\n",
    "converted_param_dict = {\n",
    "    \"backbone_model\": [],\n",
    "    \"tuning_method\": [],\n",
    "    \"trainable_param\": []\n",
    "}\n",
    "\n",
    "# Iterate through the original dictionary to populate the new one\n",
    "for model, approaches in param_dict.items():\n",
    "    for approach, param in approaches.items():\n",
    "        converted_param_dict[\"backbone_model\"].append(model)\n",
    "        converted_param_dict[\"tuning_method\"].append(approach)\n",
    "        converted_param_dict[\"trainable_param\"].append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>trainable_param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>30728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>184320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>38400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>884736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>222882048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>589824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>37128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>491520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>2359296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>737639424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  trainable_param\n",
       "0    codet5p-220m  Transducer Tuning            30728\n",
       "1    codet5p-220m      Prefix-Tuning           184320\n",
       "2    codet5p-220m      Prompt-Tuning            38400\n",
       "3    codet5p-220m               LoRA           884736\n",
       "4    codet5p-220m   Full Fine-Tuning        222882048\n",
       "5    codet5p-220m     Linear Adapter           589824\n",
       "6    codet5p-770m  Transducer Tuning            37128\n",
       "7    codet5p-770m      Prefix-Tuning           491520\n",
       "8    codet5p-770m      Prompt-Tuning           102400\n",
       "9    codet5p-770m               LoRA          2359296\n",
       "10   codet5p-770m   Full Fine-Tuning        737639424\n",
       "11   codet5p-770m     Linear Adapter          1048576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage = pd.DataFrame(converted_param_dict)\n",
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>bleu-cn_mean</th>\n",
       "      <th>bleu-cn_std</th>\n",
       "      <th>trainable_param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>99.84</td>\n",
       "      <td>0.21</td>\n",
       "      <td>30728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>222882048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>884736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>95.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>98.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>589824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>99.93</td>\n",
       "      <td>0.01</td>\n",
       "      <td>184320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>38400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>98.11</td>\n",
       "      <td>1.61</td>\n",
       "      <td>37128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>99.81</td>\n",
       "      <td>0.01</td>\n",
       "      <td>737639424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>99.79</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2359296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>87.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>98.24</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>99.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>491520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>99.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  bleu-cn_mean  bleu-cn_std  \\\n",
       "0    codet5p-220m  Transducer Tuning         99.84         0.21   \n",
       "1    codet5p-220m   Full Fine-Tuning         99.91         0.01   \n",
       "2    codet5p-220m               LoRA         99.91         0.00   \n",
       "3    codet5p-220m     No Fine-Tuning         95.49         0.00   \n",
       "4    codet5p-220m     Linear Adapter         98.05         0.88   \n",
       "5    codet5p-220m      Prefix-Tuning         99.93         0.01   \n",
       "6    codet5p-220m      Prompt-Tuning         99.91         0.01   \n",
       "7    codet5p-770m  Transducer Tuning         98.11         1.61   \n",
       "8    codet5p-770m   Full Fine-Tuning         99.81         0.01   \n",
       "9    codet5p-770m               LoRA         99.79         0.02   \n",
       "10   codet5p-770m     No Fine-Tuning         87.90         0.00   \n",
       "11   codet5p-770m     Linear Adapter         98.24         1.39   \n",
       "12   codet5p-770m      Prefix-Tuning         99.85         0.00   \n",
       "13   codet5p-770m      Prompt-Tuning         99.82         0.01   \n",
       "\n",
       "    trainable_param  \n",
       "0           30728.0  \n",
       "1       222882048.0  \n",
       "2          884736.0  \n",
       "3               0.0  \n",
       "4          589824.0  \n",
       "5          184320.0  \n",
       "6           38400.0  \n",
       "7           37128.0  \n",
       "8       737639424.0  \n",
       "9         2359296.0  \n",
       "10              0.0  \n",
       "11        1048576.0  \n",
       "12         491520.0  \n",
       "13         102400.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metric = pd.merge(left=df_metric, right=usage, on=[\"backbone_model\", \"tuning_method\"], how=\"outer\")\n",
    "df_metric.fillna(0, inplace=True)\n",
    "df_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summarization/codet5p-220m_prompt-tuning.csv',\n",
       " 'summarization/codet5p-220m_concatpervector.csv',\n",
       " 'summarization/codet5p-770m_full-finetuning.csv',\n",
       " 'summarization/codet5p-770m_lora.csv',\n",
       " 'summarization/codet5p-220m_prefix-tuning.csv',\n",
       " 'summarization/codet5p-770m_prefix-tuning.csv',\n",
       " 'summarization/codet5p-770m_no-finetuning.csv',\n",
       " 'summarization/codet5p-770m_concatpervector.csv',\n",
       " 'summarization/codet5p-220m_no-finetuning.csv',\n",
       " 'summarization/codet5p-220m_full-finetuning.csv',\n",
       " 'summarization/codet5p-220m_lora.csv',\n",
       " 'summarization/codet5p-220m_no-gnn.csv',\n",
       " 'summarization/codet5p-770m_no-gnn.csv',\n",
       " 'summarization/codet5p-770m_prompt-tuning.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(root_path: str)->list:\n",
    "    output_paths = []\n",
    "    for filename in os.listdir(root_path):\n",
    "        filepath = os.path.join(root_path, filename)\n",
    "        output_paths.append(filepath)\n",
    "    return output_paths\n",
    "\n",
    "output_paths = read_csv(TASK)\n",
    "output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_for_stat_test(paths: list)->pd.DataFrame:\n",
    "    gaft_dict = {}\n",
    "    baseline_dict = {}\n",
    "    for path in paths:\n",
    "        if \"ipynb_checkpoints\" not in path:\n",
    "            filename = os.path.basename(path)\n",
    "            filename = filename.split(\"_\")\n",
    "            backbone_model = filename[0]\n",
    "            tuning_method = os.path.splitext(filename[1])[0]\n",
    "    \n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            # filter seed and similar ids\n",
    "            temp_task = TASK if TASK != \"code_repair\" else \"code_repair_long\"\n",
    "            ids_path = os.path.join(DATASET_BASEPATH, f\"{temp_task}/included_ids.csv\")\n",
    "            included_ids = pd.read_csv(ids_path)\n",
    "            mask_ids = df[\"idx.1\"].isin(included_ids[\"idx\"])\n",
    "            mask_seed = df[\"seed\"].isin(SEEDS)\n",
    "            df = df[(mask_seed) & (mask_ids)].copy()\n",
    "            if tuning_method == \"concatpervector\":\n",
    "                if backbone_model not in gaft_dict:\n",
    "                    gaft_dict[backbone_model] = {}\n",
    "                gaft_dict[backbone_model][tuning_method] = df\n",
    "            else:\n",
    "                if backbone_model not in baseline_dict:\n",
    "                    baseline_dict[backbone_model] = {}\n",
    "                baseline_dict[backbone_model][tuning_method] = df\n",
    "    return gaft_dict, baseline_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaft_dict, baseline_dict = create_dict_for_stat_test(output_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Flips the buffer output buffer so we can start reading bytes from it. If we are starting to drain because there was overflow and there aren t actually any characters to drain then the overflow must be due to a small output buffer. If we are starting to drain because there was overflow and there aren t actually any characters to drain then the overflow must be due to a small output buffer. If we are starting to drain because there was overflow and there aren t actually any characters to drain then the overflow must be due to a small output buffer. If we are starting to drain because there was overflow and there aren t actually any characters to drain then the overflow must be due to a small output buffer. If we are starting to drain because there was overflow and there aren t actually any characters to drain then the overflow must be due to a small output buffer. If',\n",
       " 'Flips the buffer output buffer so we can start reading bytes from it. If we are starting to drain because there was overflow and there aren t actually any characters to drain then the overflow must be due to a small output buffer.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = gaft_dict[\"codet5p-770m\"][\"concatpervector\"]\n",
    "idx = 0\n",
    "temp[temp[\"bleu-cn\"] < 100].iloc[idx].preds, temp[temp[\"bleu-cn\"] < 100].iloc[idx].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_wilcoxon_test(gaft_dict, baseline_dict, metric_name):\n",
    "    p_val_dict = {}\n",
    "    r_val_dict = {}\n",
    "    for backbone_model, tuning_method_dict in baseline_dict.items():\n",
    "        df_reference = gaft_dict[backbone_model][\"concatpervector\"]\n",
    "        group1 = df_reference[metric_name].to_list()\n",
    "        p_val_dict[backbone_model] = {}\n",
    "        r_val_dict[backbone_model] = {}\n",
    "        for tuning_method, df_baseline in tuning_method_dict.items():    \n",
    "            group2 = df_baseline[metric_name].to_list()\n",
    "            assert len(group1) == len(group2)\n",
    "            print(f\"Performing test | {len(group1)} samples | {backbone_model} | concatpervector vs {tuning_method}\")\n",
    "\n",
    "            w, alternative, p, rbc, CLES = wilcoxon(group1, group2, **{\"zero_method\": \"zsplit\"}).iloc[0].to_list()\n",
    "\n",
    "            # w, alternative, p = wilcoxon(group1, group2, alternative=\"two-sided\", zero_method=\"zsplit\")\n",
    "            p_val_dict[backbone_model][tuning_method] = p\n",
    "            r_val_dict[backbone_model][tuning_method] = rbc\n",
    "\n",
    "    return p_val_dict, r_val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MAPPING_INVERSE = {v:k for k,v in NAME_MAPPING.items()}\n",
    "RENAME_TUNING_METHOD_INVERSE = {v:k for k,v in RENAME_TUNING_METHOD_DICT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(df_input, p_val_dict, col_name):\n",
    "    df = df_input.copy()\n",
    "    df[col_name] = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        backbone_model = row[\"backbone_model\"]\n",
    "        tuning_method = RENAME_TUNING_METHOD_INVERSE[row[\"tuning_method\"]]\n",
    "        if tuning_method in p_val_dict.get(backbone_model, {}):\n",
    "            df.at[idx, col_name] = p_val_dict[backbone_model][tuning_method]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing test | 6296 samples | codet5p-220m | concatpervector vs prompt-tuning\n",
      "Performing test | 6296 samples | codet5p-220m | concatpervector vs prefix-tuning\n",
      "Performing test | 6296 samples | codet5p-220m | concatpervector vs no-finetuning\n",
      "Performing test | 6296 samples | codet5p-220m | concatpervector vs full-finetuning\n",
      "Performing test | 6296 samples | codet5p-220m | concatpervector vs lora\n",
      "Performing test | 6296 samples | codet5p-220m | concatpervector vs no-gnn\n",
      "Performing test | 6296 samples | codet5p-770m | concatpervector vs full-finetuning\n",
      "Performing test | 6296 samples | codet5p-770m | concatpervector vs lora\n",
      "Performing test | 6296 samples | codet5p-770m | concatpervector vs prefix-tuning\n",
      "Performing test | 6296 samples | codet5p-770m | concatpervector vs no-finetuning\n",
      "Performing test | 6296 samples | codet5p-770m | concatpervector vs no-gnn\n",
      "Performing test | 6296 samples | codet5p-770m | concatpervector vs prompt-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2230464/1450134502.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.7832763206879751' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, col_name] = p_val_dict[backbone_model][tuning_method]\n",
      "/tmp/ipykernel_2230464/1450134502.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '-0.24124293785310735' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, col_name] = p_val_dict[backbone_model][tuning_method]\n"
     ]
    }
   ],
   "source": [
    "p_val_dict, r_val_dict = perform_wilcoxon_test(gaft_dict, baseline_dict, \"bleu-cn\")\n",
    "df_final = combine_df(df_metric, p_val_dict, \"p_val\")\n",
    "df_final = combine_df(df_final, r_val_dict, \"rbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>trainable_param</th>\n",
       "      <th>bleu-cn_mean</th>\n",
       "      <th>bleu-cn_std</th>\n",
       "      <th>p_val</th>\n",
       "      <th>rbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>30728.0</td>\n",
       "      <td>99.84</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>222882048.0</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.832763e-01</td>\n",
       "      <td>-0.241243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>884736.0</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.074675e-01</td>\n",
       "      <td>-0.340121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.297460e-18</td>\n",
       "      <td>0.999647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>589824.0</td>\n",
       "      <td>98.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4.903016e-05</td>\n",
       "      <td>0.915775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>184320.0</td>\n",
       "      <td>99.93</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.641093e-01</td>\n",
       "      <td>-0.266511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>38400.0</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.832712e-01</td>\n",
       "      <td>-0.239564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>37128.0</td>\n",
       "      <td>98.11</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>737639424.0</td>\n",
       "      <td>99.81</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.365281e-29</td>\n",
       "      <td>-0.773461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>2359296.0</td>\n",
       "      <td>99.79</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.049765e-29</td>\n",
       "      <td>-0.772493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.711198e-79</td>\n",
       "      <td>0.993580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>1048576.0</td>\n",
       "      <td>98.24</td>\n",
       "      <td>1.39</td>\n",
       "      <td>4.046832e-13</td>\n",
       "      <td>-0.253904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>491520.0</td>\n",
       "      <td>99.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.790241e-29</td>\n",
       "      <td>-0.775051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>102400.0</td>\n",
       "      <td>99.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.104023e-30</td>\n",
       "      <td>-0.775879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  trainable_param  bleu-cn_mean  \\\n",
       "0    CodeT5+ 220M  Transducer Tuning          30728.0         99.84   \n",
       "1    CodeT5+ 220M   Full Fine-Tuning      222882048.0         99.91   \n",
       "2    CodeT5+ 220M               LoRA         884736.0         99.91   \n",
       "3    CodeT5+ 220M     No Fine-Tuning              0.0         95.49   \n",
       "4    CodeT5+ 220M     Linear Adapter         589824.0         98.05   \n",
       "5    CodeT5+ 220M      Prefix-Tuning         184320.0         99.93   \n",
       "6    CodeT5+ 220M      Prompt-Tuning          38400.0         99.91   \n",
       "7    CodeT5+ 770M  Transducer Tuning          37128.0         98.11   \n",
       "8    CodeT5+ 770M   Full Fine-Tuning      737639424.0         99.81   \n",
       "9    CodeT5+ 770M               LoRA        2359296.0         99.79   \n",
       "10   CodeT5+ 770M     No Fine-Tuning              0.0         87.90   \n",
       "11   CodeT5+ 770M     Linear Adapter        1048576.0         98.24   \n",
       "12   CodeT5+ 770M      Prefix-Tuning         491520.0         99.85   \n",
       "13   CodeT5+ 770M      Prompt-Tuning         102400.0         99.82   \n",
       "\n",
       "    bleu-cn_std         p_val       rbc  \n",
       "0          0.21  0.000000e+00  0.000000  \n",
       "1          0.01  7.832763e-01 -0.241243  \n",
       "2          0.00  7.074675e-01 -0.340121  \n",
       "3          0.00  6.297460e-18  0.999647  \n",
       "4          0.88  4.903016e-05  0.915775  \n",
       "5          0.01  7.641093e-01 -0.266511  \n",
       "6          0.01  7.832712e-01 -0.239564  \n",
       "7          1.61  0.000000e+00  0.000000  \n",
       "8          0.01  1.365281e-29 -0.773461  \n",
       "9          0.02  1.049765e-29 -0.772493  \n",
       "10         0.00  9.711198e-79  0.993580  \n",
       "11         1.39  4.046832e-13 -0.253904  \n",
       "12         0.00  1.790241e-29 -0.775051  \n",
       "13         0.01  6.104023e-30 -0.775879  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = ['backbone_model', 'tuning_method', 'trainable_param', \n",
    "            'bleu-cn_mean', 'bleu-cn_std','p_val', 'rbc'\n",
    "        ]\n",
    "df_final = df_final[ordering].copy()\n",
    "df_final[\"backbone_model\"] = df_final[\"backbone_model\"].apply(lambda x: NAME_MAPPING[x]) \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(f\"t_test_{TASK}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
