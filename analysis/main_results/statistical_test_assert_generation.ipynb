{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pingouin import wilcoxon\n",
    "from tqdm import tqdm\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"assert_generation\"\n",
    "NAME_MAPPING = {\n",
    "    \"codet5p-220m\": \"CodeT5+ 220M\",\n",
    "    \"codet5p-770m\": \"CodeT5+ 770M\"\n",
    "}\n",
    "RENAME_TUNING_METHOD_DICT = {\n",
    "    \"full-finetuning\": \"Full Fine-Tuning\",\n",
    "    \"no-gnn\": \"Linear Adapter\",\n",
    "    \"concatpervector\": \"Transducer Tuning\",\n",
    "    \"lora\": \"LoRA\",\n",
    "    \"prompt-tuning\": \"Prompt-Tuning\",\n",
    "    \"prefix-tuning\": \"Prefix-Tuning\",\n",
    "    \"no-finetuning\": \"No Fine-Tuning\"\n",
    "}\n",
    "\n",
    "SEEDS = (\"seed_18_1\", \"seed_99_1\")\n",
    "DATASET_BASEPATH = \"/data/datasets/fix/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assert_generation/codet5p-220m_prompt-tuning.csv',\n",
       " 'assert_generation/codet5p-220m_concatpervector.csv',\n",
       " 'assert_generation/codet5p-770m_full-finetuning.csv',\n",
       " 'assert_generation/codet5p-770m_lora.csv',\n",
       " 'assert_generation/codet5p-220m_prefix-tuning.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(root_path: str)->list:\n",
    "    output_paths = []\n",
    "    for filename in os.listdir(root_path):\n",
    "        filepath = os.path.join(root_path, filename)\n",
    "        output_paths.append(filepath)\n",
    "    return output_paths\n",
    "\n",
    "output_paths = read_csv(TASK)\n",
    "output_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(paths: list)->pd.DataFrame:\n",
    "    temp_list = []\n",
    "    for path in paths:\n",
    "        if \"ipynb_checkpoints\" not in path:\n",
    "            filename = os.path.basename(path)\n",
    "            filename = filename.split(\"_\")\n",
    "            model = filename[0]\n",
    "            if model in NAME_MAPPING.keys():\n",
    "                \n",
    "                df = pd.read_csv(path)\n",
    "\n",
    "                task = path.split(\"/\")[0]\n",
    "                df[\"task\"] = task\n",
    "\n",
    "                # filter seed and similar ids\n",
    "                temp_task = task if task != \"code_repair\" else \"code_repair_long\"\n",
    "                ids_path = os.path.join(DATASET_BASEPATH, f\"{temp_task}/included_ids.csv\")\n",
    "                included_ids = pd.read_csv(ids_path)\n",
    "                mask_ids = df[\"idx.1\"].isin(included_ids[\"idx\"])\n",
    "                mask_seed = df[\"seed\"].isin(SEEDS)\n",
    "                df = df[(mask_seed) & (mask_ids)].copy()\n",
    "                df[\"model\"] = model\n",
    "\n",
    "                tuning_method = os.path.splitext(filename[1])[0]\n",
    "                df[\"tuning_method\"] = tuning_method\n",
    "\n",
    "                if task != \"summarization\":\n",
    "                    # df.drop(columns=[\"codebleu_stat\"], inplace=True)\n",
    "                    df = df[[\"model\", \"tuning_method\", \"task\", \"seed\", \"codebleu-cn\"]].copy()\n",
    "                    df.rename(columns={\"codebleu-cn\":\"codebleu\"}, inplace=True)\n",
    "                else:\n",
    "                    df = df[[\"model\", \"tuning_method\", \"task\",  \"seed\", \"bleu-cn\"]].copy().round(2)\n",
    "\n",
    "                temp_list.append(df)\n",
    "\n",
    "    df = pd.concat(temp_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>codebleu_mean</th>\n",
       "      <th>codebleu_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>82.32</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>83.16</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>76.85</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>82.48</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>81.16</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>83.16</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>74.13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>81.23</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>83.15</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  codebleu_mean  codebleu_std\n",
       "0    codet5p-220m  Transducer Tuning          82.32          0.30\n",
       "1    codet5p-220m   Full Fine-Tuning          83.16          0.01\n",
       "2    codet5p-220m               LoRA          83.17          0.00\n",
       "3    codet5p-220m     No Fine-Tuning          76.85          0.00\n",
       "4    codet5p-220m     Linear Adapter          82.48          0.02\n",
       "5    codet5p-220m      Prefix-Tuning          83.17          0.00\n",
       "6    codet5p-220m      Prompt-Tuning          83.17          0.00\n",
       "7    codet5p-770m  Transducer Tuning          81.16          0.71\n",
       "8    codet5p-770m   Full Fine-Tuning          83.16          0.01\n",
       "9    codet5p-770m               LoRA          83.17          0.00\n",
       "10   codet5p-770m     No Fine-Tuning          74.13          0.00\n",
       "11   codet5p-770m     Linear Adapter          81.23          0.88\n",
       "12   codet5p-770m      Prefix-Tuning          83.15          0.02\n",
       "13   codet5p-770m      Prompt-Tuning          83.17          0.00"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_df(output_paths)\n",
    "\n",
    "temp_df = df.groupby([\"model\", \"tuning_method\", \"task\", \"seed\"], as_index=False).mean()\n",
    "temp_std = temp_df.groupby([\"model\", \"tuning_method\", \"task\"], as_index=False)[\"codebleu\"].std().round(2)\n",
    "\n",
    "df.drop(columns=[\"seed\"], inplace=True)\n",
    "\n",
    "# Calculate the mean and standard deviation for each group\n",
    "temp_mean = df.groupby([\"model\", \"tuning_method\", \"task\"], as_index=False).mean().round(2)\n",
    "\n",
    "# Add a suffix to the columns to distinguish between mean and std\n",
    "temp_mean = temp_mean.add_suffix('_mean')\n",
    "temp_std = temp_std.add_suffix('_std')\n",
    "\n",
    "# Merge mean and std DataFrames\n",
    "df_metric = pd.merge(temp_mean, temp_std, left_on=[\"model_mean\", \"tuning_method_mean\", \"task_mean\"], \n",
    "                         right_on=[\"model_std\", \"tuning_method_std\", \"task_std\"])\n",
    "\n",
    "# Drop redundant columns after merge\n",
    "df_metric.drop(columns=[\"model_std\", \"tuning_method_std\", \"task_std\", \"task_mean\"], inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_metric.rename(columns={\"model_mean\": \"backbone_model\", \"tuning_method_mean\": \"tuning_method\"} , inplace=True)\n",
    "df_metric[\"tuning_method\"] = df_metric[\"tuning_method\"].apply(lambda x: RENAME_TUNING_METHOD_DICT[x])\n",
    "\n",
    "\n",
    "df_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Trainable Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"codet5p-220m\": {\n",
    "        \"Transducer Tuning\": 30728,\n",
    "        \"Prefix-Tuning\": 184320,\n",
    "        \"Prompt-Tuning\": 38400,\n",
    "        \"LoRA\": 884736,\n",
    "        \"Full Fine-Tuning\": 222882048,\n",
    "        \"Linear Adapter\": 589824\n",
    "    },\n",
    "    \"codet5p-770m\": {\n",
    "        \"Transducer Tuning\": 37128,\n",
    "        \"Prefix-Tuning\": 491520,\n",
    "        \"Prompt-Tuning\": 102400,\n",
    "        \"LoRA\": 2359296,\n",
    "        \"Full Fine-Tuning\": 737639424,\n",
    "        \"Linear Adapter\": 1048576\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new dictionary\n",
    "converted_param_dict = {\n",
    "    \"backbone_model\": [],\n",
    "    \"tuning_method\": [],\n",
    "    \"trainable_param\": []\n",
    "}\n",
    "\n",
    "# Iterate through the original dictionary to populate the new one\n",
    "for model, approaches in param_dict.items():\n",
    "    for approach, param in approaches.items():\n",
    "        converted_param_dict[\"backbone_model\"].append(model)\n",
    "        converted_param_dict[\"tuning_method\"].append(approach)\n",
    "        converted_param_dict[\"trainable_param\"].append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>trainable_param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>30728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>184320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>38400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>884736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>222882048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>589824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>37128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>491520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>2359296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>737639424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  trainable_param\n",
       "0    codet5p-220m  Transducer Tuning            30728\n",
       "1    codet5p-220m      Prefix-Tuning           184320\n",
       "2    codet5p-220m      Prompt-Tuning            38400\n",
       "3    codet5p-220m               LoRA           884736\n",
       "4    codet5p-220m   Full Fine-Tuning        222882048\n",
       "5    codet5p-220m     Linear Adapter           589824\n",
       "6    codet5p-770m  Transducer Tuning            37128\n",
       "7    codet5p-770m      Prefix-Tuning           491520\n",
       "8    codet5p-770m      Prompt-Tuning           102400\n",
       "9    codet5p-770m               LoRA          2359296\n",
       "10   codet5p-770m   Full Fine-Tuning        737639424\n",
       "11   codet5p-770m     Linear Adapter          1048576"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage = pd.DataFrame(converted_param_dict)\n",
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>codebleu_mean</th>\n",
       "      <th>codebleu_std</th>\n",
       "      <th>trainable_param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>82.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>30728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>83.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>222882048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>884736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>76.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>82.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>589824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>184320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codet5p-220m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>81.16</td>\n",
       "      <td>0.71</td>\n",
       "      <td>37128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>83.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>737639424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2359296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>74.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>81.23</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>83.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>491520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>codet5p-770m</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  codebleu_mean  codebleu_std  \\\n",
       "0    codet5p-220m  Transducer Tuning          82.32          0.30   \n",
       "1    codet5p-220m   Full Fine-Tuning          83.16          0.01   \n",
       "2    codet5p-220m               LoRA          83.17          0.00   \n",
       "3    codet5p-220m     No Fine-Tuning          76.85          0.00   \n",
       "4    codet5p-220m     Linear Adapter          82.48          0.02   \n",
       "5    codet5p-220m      Prefix-Tuning          83.17          0.00   \n",
       "6    codet5p-220m      Prompt-Tuning          83.17          0.00   \n",
       "7    codet5p-770m  Transducer Tuning          81.16          0.71   \n",
       "8    codet5p-770m   Full Fine-Tuning          83.16          0.01   \n",
       "9    codet5p-770m               LoRA          83.17          0.00   \n",
       "10   codet5p-770m     No Fine-Tuning          74.13          0.00   \n",
       "11   codet5p-770m     Linear Adapter          81.23          0.88   \n",
       "12   codet5p-770m      Prefix-Tuning          83.15          0.02   \n",
       "13   codet5p-770m      Prompt-Tuning          83.17          0.00   \n",
       "\n",
       "    trainable_param  \n",
       "0           30728.0  \n",
       "1       222882048.0  \n",
       "2          884736.0  \n",
       "3               0.0  \n",
       "4          589824.0  \n",
       "5          184320.0  \n",
       "6           38400.0  \n",
       "7           37128.0  \n",
       "8       737639424.0  \n",
       "9         2359296.0  \n",
       "10              0.0  \n",
       "11        1048576.0  \n",
       "12         491520.0  \n",
       "13         102400.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metric = pd.merge(left=df_metric, right=usage, on=[\"backbone_model\", \"tuning_method\"], how=\"outer\")\n",
    "df_metric.fillna(0, inplace=True)\n",
    "df_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assert_generation/codet5p-220m_prompt-tuning.csv',\n",
       " 'assert_generation/codet5p-220m_concatpervector.csv',\n",
       " 'assert_generation/codet5p-770m_full-finetuning.csv',\n",
       " 'assert_generation/codet5p-770m_lora.csv',\n",
       " 'assert_generation/codet5p-220m_prefix-tuning.csv',\n",
       " 'assert_generation/codet5p-770m_prefix-tuning.csv',\n",
       " 'assert_generation/codet5p-770m_no-finetuning.csv',\n",
       " 'assert_generation/codet5p-770m_concatpervector.csv',\n",
       " 'assert_generation/codet5p-220m_no-finetuning.csv',\n",
       " 'assert_generation/codet5p-220m_full-finetuning.csv',\n",
       " 'assert_generation/codet5p-220m_lora.csv',\n",
       " 'assert_generation/codet5p-220m_no-gnn.csv',\n",
       " 'assert_generation/codet5p-770m_no-gnn.csv',\n",
       " 'assert_generation/codet5p-770m_prompt-tuning.csv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(root_path: str)->list:\n",
    "    output_paths = []\n",
    "    for filename in os.listdir(root_path):\n",
    "        filepath = os.path.join(root_path, filename)\n",
    "        output_paths.append(filepath)\n",
    "    return output_paths\n",
    "\n",
    "output_paths = read_csv(TASK)\n",
    "output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_for_stat_test(paths: list)->pd.DataFrame:\n",
    "    gaft_dict = {}\n",
    "    baseline_dict = {}\n",
    "    for path in paths:\n",
    "        if \"ipynb_checkpoints\" not in path:\n",
    "            filename = os.path.basename(path)\n",
    "            filename = filename.split(\"_\")\n",
    "            backbone_model = filename[0]\n",
    "            tuning_method = os.path.splitext(filename[1])[0]\n",
    "    \n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            # filter seed and similar ids\n",
    "            temp_task = TASK if TASK != \"code_repair\" else \"code_repair_long\"\n",
    "            ids_path = os.path.join(DATASET_BASEPATH, f\"{temp_task}/included_ids.csv\")\n",
    "            included_ids = pd.read_csv(ids_path)\n",
    "            mask_ids = df[\"idx.1\"].isin(included_ids[\"idx\"])\n",
    "            mask_seed = df[\"seed\"].isin(SEEDS)\n",
    "            df = df[(mask_seed) & (mask_ids)].copy()\n",
    "            if tuning_method == \"concatpervector\":\n",
    "                if backbone_model not in gaft_dict:\n",
    "                    gaft_dict[backbone_model] = {}\n",
    "                gaft_dict[backbone_model][tuning_method] = df\n",
    "            else:\n",
    "                if backbone_model not in baseline_dict:\n",
    "                    baseline_dict[backbone_model] = {}\n",
    "                baseline_dict[backbone_model][tuning_method] = df\n",
    "    return gaft_dict, baseline_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaft_dict, baseline_dict = create_dict_for_stat_test(output_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('org. junit. Assert. assertEquals ( FileSystemKind. OBJECT_STORE, s3. getKind ( ) ) org org org org',\n",
       " 'org. junit. Assert. assertEquals ( FileSystemKind. OBJECT_STORE, s3. getKind ( ) )')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = gaft_dict[\"codet5p-770m\"][\"concatpervector\"]\n",
    "idx = 0\n",
    "temp[temp[\"bleu-cn\"] < 100].iloc[idx].preds, temp[temp[\"bleu-cn\"] < 100].iloc[idx].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_wilcoxon_test(gaft_dict, baseline_dict, metric_name):\n",
    "    p_val_dict = {}\n",
    "    r_val_dict = {}\n",
    "    for backbone_model, tuning_method_dict in baseline_dict.items():\n",
    "        df_reference = gaft_dict[backbone_model][\"concatpervector\"]\n",
    "        group1 = df_reference[metric_name].to_list()\n",
    "        p_val_dict[backbone_model] = {}\n",
    "        r_val_dict[backbone_model] = {}\n",
    "        for tuning_method, df_baseline in tuning_method_dict.items():    \n",
    "            group2 = df_baseline[metric_name].to_list()\n",
    "            assert len(group1) == len(group2)\n",
    "            print(f\"Performing test | {len(group1)} samples | {backbone_model} | concatpervector vs {tuning_method}\")\n",
    "\n",
    "            w, alternative, p, rbc, CLES = wilcoxon(group1, group2, **{\"zero_method\": \"zsplit\"}).iloc[0].to_list()\n",
    "\n",
    "            # w, alternative, p = wilcoxon(group1, group2, alternative=\"two-sided\", zero_method=\"zsplit\")\n",
    "            p_val_dict[backbone_model][tuning_method] = p\n",
    "            r_val_dict[backbone_model][tuning_method] = rbc\n",
    "\n",
    "    return p_val_dict, r_val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MAPPING_INVERSE = {v:k for k,v in NAME_MAPPING.items()}\n",
    "RENAME_TUNING_METHOD_INVERSE = {v:k for k,v in RENAME_TUNING_METHOD_DICT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(df_input, p_val_dict, col_name):\n",
    "    df = df_input.copy()\n",
    "    df[col_name] = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        backbone_model = row[\"backbone_model\"]\n",
    "        tuning_method = RENAME_TUNING_METHOD_INVERSE[row[\"tuning_method\"]]\n",
    "        if tuning_method in p_val_dict.get(backbone_model, {}):\n",
    "            df.at[idx, col_name] = p_val_dict[backbone_model][tuning_method]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing test | 6710 samples | codet5p-220m | concatpervector vs prompt-tuning\n",
      "Performing test | 6710 samples | codet5p-220m | concatpervector vs prefix-tuning\n",
      "Performing test | 6710 samples | codet5p-220m | concatpervector vs no-finetuning\n",
      "Performing test | 6710 samples | codet5p-220m | concatpervector vs full-finetuning\n",
      "Performing test | 6710 samples | codet5p-220m | concatpervector vs lora\n",
      "Performing test | 6710 samples | codet5p-220m | concatpervector vs no-gnn\n",
      "Performing test | 6710 samples | codet5p-770m | concatpervector vs full-finetuning\n",
      "Performing test | 6710 samples | codet5p-770m | concatpervector vs lora\n",
      "Performing test | 6710 samples | codet5p-770m | concatpervector vs prefix-tuning\n",
      "Performing test | 6710 samples | codet5p-770m | concatpervector vs no-finetuning\n",
      "Performing test | 6710 samples | codet5p-770m | concatpervector vs no-gnn\n",
      "Performing test | 6710 samples | codet5p-770m | concatpervector vs prompt-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2215531/1450134502.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2.361745735757814e-68' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, col_name] = p_val_dict[backbone_model][tuning_method]\n",
      "/tmp/ipykernel_2215531/1450134502.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '-0.996832002717853' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, col_name] = p_val_dict[backbone_model][tuning_method]\n"
     ]
    }
   ],
   "source": [
    "p_val_dict, r_val_dict = perform_wilcoxon_test(gaft_dict, baseline_dict, \"codebleu-cn\")\n",
    "df_final = combine_df(df_metric, p_val_dict, \"p_val\")\n",
    "df_final = combine_df(df_final, r_val_dict, \"rbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone_model</th>\n",
       "      <th>tuning_method</th>\n",
       "      <th>trainable_param</th>\n",
       "      <th>codebleu_mean</th>\n",
       "      <th>codebleu_std</th>\n",
       "      <th>p_val</th>\n",
       "      <th>rbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>30728.0</td>\n",
       "      <td>82.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>222882048.0</td>\n",
       "      <td>83.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.361746e-68</td>\n",
       "      <td>-0.996832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>884736.0</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.214596e-74</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.857335e-81</td>\n",
       "      <td>0.978422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>589824.0</td>\n",
       "      <td>82.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.160066e-35</td>\n",
       "      <td>-0.645213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>184320.0</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.570560e-72</td>\n",
       "      <td>-0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CodeT5+ 220M</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>38400.0</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.348891e-72</td>\n",
       "      <td>-0.993445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Transducer Tuning</td>\n",
       "      <td>37128.0</td>\n",
       "      <td>81.16</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Full Fine-Tuning</td>\n",
       "      <td>737639424.0</td>\n",
       "      <td>83.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.979684e-29</td>\n",
       "      <td>-0.998957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>LoRA</td>\n",
       "      <td>2359296.0</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.355897e-30</td>\n",
       "      <td>-0.999783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>No Fine-Tuning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.699804e-84</td>\n",
       "      <td>0.997257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Linear Adapter</td>\n",
       "      <td>1048576.0</td>\n",
       "      <td>81.23</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.918910e-05</td>\n",
       "      <td>-0.053453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Prefix-Tuning</td>\n",
       "      <td>491520.0</td>\n",
       "      <td>83.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.892986e-28</td>\n",
       "      <td>-0.996970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CodeT5+ 770M</td>\n",
       "      <td>Prompt-Tuning</td>\n",
       "      <td>102400.0</td>\n",
       "      <td>83.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.461751e-30</td>\n",
       "      <td>-0.999797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   backbone_model      tuning_method  trainable_param  codebleu_mean  \\\n",
       "0    CodeT5+ 220M  Transducer Tuning          30728.0          82.32   \n",
       "1    CodeT5+ 220M   Full Fine-Tuning      222882048.0          83.16   \n",
       "2    CodeT5+ 220M               LoRA         884736.0          83.17   \n",
       "3    CodeT5+ 220M     No Fine-Tuning              0.0          76.85   \n",
       "4    CodeT5+ 220M     Linear Adapter         589824.0          82.48   \n",
       "5    CodeT5+ 220M      Prefix-Tuning         184320.0          83.17   \n",
       "6    CodeT5+ 220M      Prompt-Tuning          38400.0          83.17   \n",
       "7    CodeT5+ 770M  Transducer Tuning          37128.0          81.16   \n",
       "8    CodeT5+ 770M   Full Fine-Tuning      737639424.0          83.16   \n",
       "9    CodeT5+ 770M               LoRA        2359296.0          83.17   \n",
       "10   CodeT5+ 770M     No Fine-Tuning              0.0          74.13   \n",
       "11   CodeT5+ 770M     Linear Adapter        1048576.0          81.23   \n",
       "12   CodeT5+ 770M      Prefix-Tuning         491520.0          83.15   \n",
       "13   CodeT5+ 770M      Prompt-Tuning         102400.0          83.17   \n",
       "\n",
       "    codebleu_std         p_val       rbc  \n",
       "0           0.30  0.000000e+00  0.000000  \n",
       "1           0.01  2.361746e-68 -0.996832  \n",
       "2           0.00  9.214596e-74 -1.000000  \n",
       "3           0.00  5.857335e-81  0.978422  \n",
       "4           0.02  1.160066e-35 -0.645213  \n",
       "5           0.00  2.570560e-72 -0.999914  \n",
       "6           0.00  6.348891e-72 -0.993445  \n",
       "7           0.71  0.000000e+00  0.000000  \n",
       "8           0.01  4.979684e-29 -0.998957  \n",
       "9           0.00  8.355897e-30 -0.999783  \n",
       "10          0.00  3.699804e-84  0.997257  \n",
       "11          0.88  3.918910e-05 -0.053453  \n",
       "12          0.02  2.892986e-28 -0.996970  \n",
       "13          0.00  6.461751e-30 -0.999797  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = ['backbone_model', 'tuning_method', 'trainable_param', \n",
    "            'codebleu_mean', 'codebleu_std','p_val', 'rbc'\n",
    "        ]\n",
    "df_final = df_final[ordering].copy()\n",
    "df_final[\"backbone_model\"] = df_final[\"backbone_model\"].apply(lambda x: NAME_MAPPING[x]) \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(f\"t_test_{TASK}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
